---
title: "Baseball Data Analysis - Technical Summary"
output: html_document
---

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(lm.beta)
library(car)
library(knitr)
source("analyse.R")
load("data_summary.Rdata")
load("silver_slug_model.Rdata")
```

# Exploratory analysis of the data

The data set contains baseball player data from `r min(by_year$yearID)` to `r max(by_year$yearID)`. Records are maintained on each player, for each year, team, league, stint, and fielding position they played. Overall, `r player_count` unique players are recorded in the data set, covering `r man_years` man years.

```{r echo=FALSE}
ggplot(by_year, aes(x = as.factor(yearID), y = players)) +
  geom_bar(stat = "identity", fill = "Blue") +
  xlab("Year") +
  ylab("Players") +
  ggtitle("Players per Year") +
  theme(axis.text.x = element_text(angle = -60, hjust = -0))
```

## Batting Data Exploration

Batting data was recorded including:

* b_G - Games
* b_G_batting - Games as Batter
* b_AB - At Bats
* b_R - Runs
* b_H - Hits
* b_2B - Doubles
* b_3B - Triples
* b_HR - Home Runs
* b_RBI - Runs Batted In
* b_SB - Stolen Bases
* b_CS - Caught Stealing
* b_BB - Bases on Balls (Walks)
* b_SO - Strikeouts
* b_IBB - Intentional Bases on Balls (Walks)
* b_HBP - Hit By Pitch
* b_SH - Sacrifice Hits (Bunts)
* b_SF - Sacrifice Flies
* b_GIDP - Grounded into Double Plays
* b_G_old - Old version of games (depreciated) 

In addition to the baseball statistics provided, several additional ratios were computed to normalize the data:

* b_hits_per_AB - Hits per At Bat
* b_runs_per_AB - Runs per At Bat
* b_runs_per_H - Runs per Hit
* b_home_runs_per_AB - Home Runs per At Bat
* b_balls_per_AB - Balls per At Bat
* b_RBI_per_H - Runs Batted In per Hit
* b_HBP_per_AB - Hit By Pitch per At Bat
* b_games_batted_per_all_games - Games Batted per All Games Played

Finally, award data was considered:

```{r echo=FALSE}
ggplot(filter(award_winners, !is.na(awardID)), aes(x = as.factor(awardID), y = players)) +
  geom_bar(stat = "identity", fill = "Blue") +
  xlab("Award") +
  ylab("Players") +
  ggtitle("Unique Players") +
  theme(axis.text.x = element_text(angle = -60, hjust = -0))
```

As the graphs suggests, the Silver Slugger Award has the most winners contained in the data set. As a result of having the most data, it was chosen as the dependent variable.  

The data-set also contained team, league, & fielding data that was not considered.

# Model selection and Definitions
To model the likelihood that a player would received the Silver Slugger (SS) Award a Logistic Regression was performed. According to Wikipedia, The SS award is awarded annually to the best offensive player (batting) for each position in both leagues of the MLB. A single player can win the SS multiple times over a series of years. Since the SS is a batting award recognizing players of high offensive value, the batting data set was used. Voters vote for players based on several batting ratios that we were able to derive from our raw count data.

## Full Model:

```{r echo=F}
summary(model)
```

We tested both backward selection and step-wise selection routines. They both concluded with the same model - we simply used the backward selection model. We did not pursue additional selection procedures.

## Backward Selected Model

```{r echo=F}
summary(model_backwards_selection)
```

## MultiColinearity

```{r echo=F,results='asis',error=F,warning=F}
kable(vif(model_backwards_linear) %>% 
        {data.frame(Parameters = names(.), VIF = ., row.names = NULL)}, 
      format = "markdown")
```

Since we are dealing with a logistic  model, we had to create a linear "side model" for multicolinearity testing purposes based on the backward selected model. Unfortunately the VIF analysis shows that we have a plethora of multiconinearity issues. This does however make sense as much of the x variables are dependent on one another. For example, there cannot be a run without a hit (normally speaking; being walked is a possibility as well). Removing these variables did cause our AIC to increase, however this was expected as the backward selection routine was aiming to minimize AIC.

To alleviate the multicolineatity issue, the following variables were removed:

b_games_batted_per_all_games
b_G_batting
b_SF
b_HBP_per_AB
b_HBP

* Games Played (b_G)
* Games Played Where Batted (b_G_batting)
* Runs (b_R)
* Runs Batted in (b_RBI)

## Revised Backward Selected Model

```{r echo=F}
summary(model_backwards_semifinal)
```

After removing the x variables that had multicolinearity issues, we then went through and removed non significant variables using a .05 p-value threshold. The variables with the highest p-values were removed one at a time until all variables were significant. Our AIC lowered slightly as well.

## Final Model

```{r echo=F}
summary(model_backwards_final)
```

```{r include = FALSE}
print_back_selected_model <- paste(round(model_backwards_final$coefficients, digits = 2), names(model_backwards_final$coefficients), sep = " * ")
print_back_selected_model[1] <- round(model_backwards_final$coefficients[1],digits = 2)
print_back_selected_model <- paste(print_back_selected_model, collapse = " + ")
print_back_selected_model <- paste("log(odds(win_silver_slug)) = ", print_back_selected_model, sep = "")
```
>`r print_back_selected_model`

```{r echo=F,results='asis',error=F,warning=F}
kable(vif(model_backwards_final_lm) %>% 
        {data.frame(Parameters = names(.), VIF = ., row.names = NULL)}, 
      format = "markdown")
```

We now have a final model with only significant variables and have removed variables with multicolinearity:

## Check Fit

```{r echo=FALSE}
batting_data[which(colnames(batting_data) %in% c("win_silver_slug", names(model_backwards_final$coefficients)))] %>%
plot_all_box(which(colnames(.) == "win_silver_slug"), .)
```

TODO: Clean up graphs, explain graphs

## Strongest Model Predictors

```{r echo=F,results='asis',error=F,warning=F}
kable(lm.beta(model_backwards_final)$standardized.coefficients %>% 
        {data.frame(Parameters = names(.), "Standardized_Coefficients" = ., row.names = NULL)}, 
      format = "markdown")
```


TODO: Explain here which were the strongest predictors

## Leverage Points

```{r echo=FALSE}
influencePlot(model_backwards_final, labels= paste(batting_data$playerID, batting_data$yearID))
```

ToDO: add info about influential points and what looked into (bonds, steroid use)


## Computing Predictions

### Prediction 1:
```{r}
testPredict1 <- data.frame(b_H = c(150),
                           b_HR = c(30), 
                           b_SB = c(10),
                           b_BB = c(50), 
                           b_SO = c(30), 
                           b_IBB = c(10),
                           b_home_runs_per_H = c(.15), 
                           b_balls_per_AB = c(.10))
predict(model_backwards_final, newdata = testPredict1, type = "response", se.fit = T)
```

### Prediction 2:
```{r}
testPredict2 <- data.frame(b_H = c(250), 
                           b_HR = c(60), 
                           b_BB = c(40), 
                           b_SB = c(1),
                           b_SO = c(10), 
                           b_IBB = c(5),
                           b_home_runs_per_H = c(.4), 
                           b_balls_per_AB = c(.5))
predict(model_backwards_final, newdata = testPredict2, type = "response", se.fit = T)
```

ToDo: Compute two predictions here and explain

## Finding a Optimal Cut Off

Since few players win the Silver Slugger award each year, a naive prediction cut off of 0.5 would not provide good results. To find a more optimal cut off, cut off values were trialed, and the one that produced the maximum F Measure was chosen. The Optimal F Measure is marked with a vertical line on the following plot.

```{r message=FALSE, warning=FALSE, echo=FALSE}
plot_of_cut_offs
```

After searching for a good cut off value, `r best_cut_off["cut_offs"]` was chosen yielding the following results.

```{r echo=F,results='asis',error=F,warning=F}
kable(best_cut_off[,1:4], 
      format = "markdown")
```

```{r echo=F,results='asis',error=F,warning=F}
kable(best_cut_off[,5:ncol(best_cut_off)], 
      format = "markdown")
```

## Cross Validation
To validate the predictive strength of the model the following precedure was repeated acrost 5 folds, 5 times for a total of 25 trials:

1. Fit Model To Training Data
2. Determine Cut Offs Using Training Data
3. Test Model Prediction using Test Data

The following metrics were computed:

```{r echo=FALSE}
boxplot(cross_validated_results[,c("accuracy", "specificity")],
        main = "Cross Validated Metrics",
        ylab = "Value of Metric",
        xlab = "Metric")
```

We can see that the Accuracy and Specificity of the cross validated models is fairly stable and high. This is to be expected due since the vast majority of players do not win awards each year.

```{r echo=FALSE}
boxplot(cross_validated_results[,c("recall", "precision", "f_measure")],
        main = "Cross Validated Metrics",
        ylab = "Value of Metric",
        xlab = "Metric")
```

We can see that the model has a mean recall of `r round(mean(cross_validated_results$recall), digits = 2)` meaning `r round(mean(cross_validated_results$recall), digits = 2) * 100`% of the players each year that won an award were correctly labeled. Since the mean precision is `r round(mean(cross_validated_results$precision), digits = 2)`, `r round(mean(cross_validated_results$precision), digits = 2) * 100`% of the players the model predicts to win, actually do so.
